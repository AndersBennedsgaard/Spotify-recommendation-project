{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import spotipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.client import SpotifyException\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "# models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree, DecisionTreeClassifier\n",
    "\n",
    "from spotirecs import get_playlist_tracks, get_features, Playlists\n",
    "from spotirecs.utils import get_playlists\n",
    "from spotirecs.plotting import plot_features\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# pandas settings\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "pd.set_option('display.max_columns', 0)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_json(data: dict, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_id = '113586775'\n",
    "\n",
    "playlist_of_interest_name = 'Favorites'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logging in\n",
    "\n",
    "spotify = spotipy.Spotify(requests_timeout=10, client_credentials_manager=SpotifyClientCredentials())\n",
    "\n",
    "# Which playlists do I have?\n",
    "playlists = get_playlists(spotify, username_id)\n",
    "\n",
    "playlist_of_interest = None\n",
    "\n",
    "playlists_of_no_interest = []\n",
    "for playlist in playlists.items:\n",
    "    if playlist.name == playlist_of_interest_name:\n",
    "        playlist_of_interest = playlist\n",
    "    elif playlist.owner.id == username_id:\n",
    "        playlists_of_no_interest.append(playlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Tracks\nprevious\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='https://api.spotify.com/...&additional_types=track', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/int_parsing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven playlist name can not be found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m good_track_ids, good_track_names \u001b[38;5;241m=\u001b[39m \u001b[43mget_playlist_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspotify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplaylist_of_interest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m bad_track_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m bad_track_names \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/ml-projects/Spotify-recommendation-project/spotirecs/utils.py:29\u001b[0m, in \u001b[0;36mget_playlist_tracks\u001b[0;34m(spotify, playlist)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo tracks in user playlist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m track_results \u001b[38;5;241m=\u001b[39m \u001b[43mTracks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrack_results_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# TODO: return track objects instead of IDs\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m track_results\u001b[38;5;241m.\u001b[39mitems:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spotirecs-G7pVh_ub-py3.10/lib/python3.10/site-packages/pydantic/main.py:171\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    170\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Tracks\nprevious\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='https://api.spotify.com/...&additional_types=track', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/int_parsing"
     ]
    }
   ],
   "source": [
    "if playlist_of_interest is None:\n",
    "    print(\"Given playlist name can not be found\")\n",
    "    sys.exit(1)\n",
    "\n",
    "good_track_ids, good_track_names = get_playlist_tracks(spotify, playlist_of_interest)\n",
    "\n",
    "bad_track_ids = []\n",
    "bad_track_names = []\n",
    "\n",
    "for playlist in playlists_of_no_interest:\n",
    "    tmp_ids, tmp_names = get_playlist_tracks(spotify, playlist)\n",
    "    \n",
    "    for tmp_id, tmp_name in zip(tmp_ids, tmp_names):\n",
    "        if tmp_id not in good_track_ids and tmp_id not in bad_track_ids:\n",
    "            bad_track_ids.append(tmp_id)\n",
    "            bad_track_names.append(tmp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [1] * len(good_track_ids) + [0] * len(bad_track_ids)\n",
    "track_ids = good_track_ids + bad_track_ids\n",
    "track_names = good_track_names + bad_track_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"track_features.csv\"\n",
    "\n",
    "features = get_features(spotify, track_ids)\n",
    "favorites_df = pd.DataFrame(features, index=track_names)\n",
    "favorites_df['rating'] = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(data_file):\n",
    "    # If the data file exists, look for new tracks and append their features\n",
    "    print(\"Audio features found\")\n",
    "    read_df = pd.read_csv(data_file, index_col=0)\n",
    "    read_ids = read_df['id']\n",
    "    \n",
    "    print(\"\\tFinding new tracks ...\")\n",
    "    tmp_indices = []\n",
    "    for i, track_id in enumerate(track_ids):\n",
    "        if track_id not in read_ids.values:\n",
    "            tmp_indices.append(i)\n",
    "\n",
    "    new_ids = [track_ids[i] for i in tmp_indices]\n",
    "    new_names = [track_names[i] for i in tmp_indices]\n",
    "    new_ratings = [ratings[i] for i in tmp_indices]\n",
    "\n",
    "    new_features = get_features(spotify, new_ids)\n",
    "    new_features_df = pd.DataFrame(new_features, index=new_names)\n",
    "    new_features_df['rating'] = new_ratings\n",
    "    \n",
    "    if new_features_df.size > 0:\n",
    "        print(\"\\tAppending new track features to main file ...\")\n",
    "        new_features_df.to_csv(data_file, mode='a')\n",
    "\n",
    "        print(\"\\tCreating audio features dataframe ...\")\n",
    "        read_df = pd.concat([read_df, new_features_df])\n",
    "        favorites_df = read_df[read_df['id'].isin(track_ids)]\n",
    "\n",
    "favorites_df.to_csv(data_file)\n",
    "\n",
    "print(\"Done!\")\n",
    "favorites_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = favorites_df[[\"acousticness\", \"danceability\", \"duration_ms\", \"energy\", \"instrumentalness\",  \"key\", \"liveness\", \"loudness\", \"mode\", \"speechiness\", \"tempo\", \"valence\", \"rating\"]]\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = training_df.columns[:-1]\n",
    "\n",
    "ratings = training_df['rating'].to_numpy()\n",
    "training_array = training_df.to_numpy()\n",
    "\n",
    "fig, _ = plot_features(column_names, training_array[ratings == 0], training_array[ratings == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = training_df.drop('rating', axis=1)\n",
    "y_train = training_df['rating']\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X_train)\n",
    "pca = decomposition.PCA().fit(X_scaled)\n",
    "\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "cum_var = np.cumsum(variance_ratio)\n",
    "threshold = 0.95\n",
    "n_components = next(i for i, v in enumerate(cum_var) if v > threshold) + 1\n",
    "print(f\"Number of components for {threshold*100:.0f}% variance: {n_components}\")\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(1,13), cum_var, lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total variance explained')\n",
    "plt.xlim(1, 12)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.axvline(n_components, c='k')\n",
    "plt.axhline(threshold, c='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit your dataset to the optimal pca\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "column_names = [f\"Component #{i + 1}\" for i in range(n_components)]\n",
    "plot_features(column_names, X_train_pca[y_train == 0], X_train_pca[y_train == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 6), max_features=10000)\n",
    "X_names_sparse = v.fit_transform(track_names)\n",
    "X_names_sparse.shape\n",
    "\n",
    "X_train = sparse.csr_matrix(sparse.hstack([X_train_pca, X_names_sparse]))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-nearest neighbor classification\n",
    "n_splits = 5\n",
    "max_neighbors = 50\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "knc_params = {'n_neighbors': range(1, max_neighbors + 1)}\n",
    "knc = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "knc_grid = GridSearchCV(knc, knc_params, n_jobs=-1, cv=skf, verbose=1)\n",
    "knc_grid.fit(X_train, y_train)\n",
    "print(\"Best score: \", knc_grid.best_score_)\n",
    "\n",
    "grid_results = pd.DataFrame(knc_grid.cv_results_)\n",
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for n in range(n_splits):\n",
    "    plt.plot(grid_results['param_n_neighbors'], grid_results[f'split{n}_test_score'], label=f'Split {n}', ls='--', lw=1)\n",
    "plt.plot(grid_results['param_n_neighbors'], grid_results['mean_test_score'], label='Mean', lw=3)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Test score')\n",
    "plt.xlim(0, max_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classification\n",
    "\n",
    "rfc_parameters = {\n",
    "    'max_features': [4, 6, 8, 10], \n",
    "    'min_samples_leaf': [1, 3, 5, 7],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "forest_grid = GridSearchCV(rfc, rfc_parameters, n_jobs=-1, cv=skf, verbose=1)\n",
    "forest_grid.fit(X_train, y_train)\n",
    "print(\"Best score: \", forest_grid.best_score_)\n",
    "\n",
    "grid_results = pd.DataFrame(forest_grid.cv_results_)\n",
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for n in range(n_splits):\n",
    "    plt.plot(grid_results[f'split{n}_test_score'], label=f'Split {n}', ls='--', lw=1)\n",
    "plt.plot(grid_results['mean_test_score'], label='Mean', lw=3)\n",
    "plt.legend()\n",
    "plt.xlabel('Parameters')\n",
    "plt.ylabel('Test score')\n",
    "plt.xlim(0, len(grid_results.index) - 1)\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decision tree classification\n",
    "\n",
    "tree_parameters = {\n",
    "    'max_depth': range(1,11),\n",
    "    'max_features': range(4, 11)\n",
    "}\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_grid = GridSearchCV(tree, tree_parameters, cv=skf, n_jobs=-1, verbose=True)\n",
    "tree_grid.fit(X_train, y_train)\n",
    "print(\"Best score: \", tree_grid.best_score_)\n",
    "\n",
    "grid_results = pd.DataFrame(tree_grid.cv_results_)\n",
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for n in range(n_splits):\n",
    "    plt.plot(grid_results[f'split{n}_test_score'], label=f'Split {n}', ls='--', lw=1)\n",
    "plt.plot(grid_results['mean_test_score'], label='Mean', lw=3)\n",
    "plt.legend()\n",
    "plt.xlabel('Parameters')\n",
    "plt.ylabel('Test score')\n",
    "plt.xlim(0, len(grid_results.index) - 1)\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(tree_grid.best_estimator_, fontsize=10)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_tracks_per_track = 1\n",
    "max_training_size = 50\n",
    "\n",
    "rec_tracks = []\n",
    "for i in favorites_df['id']:\n",
    "    try:\n",
    "        rec_tracks.extend(spotify.recommendations(seed_tracks=[i], limit=rec_tracks_per_track)['tracks'])\n",
    "    except:\n",
    "        break\n",
    "\n",
    "    if len(rec_tracks) >= max_training_size:\n",
    "        rec_tracks = rec_tracks[:max_training_size]\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_track_ids = []\n",
    "rec_track_names = []\n",
    "for i in rec_tracks:\n",
    "    rec_track_ids.append(i['id'])\n",
    "    rec_track_names.append(i['name'])\n",
    "\n",
    "rec_features = get_features(spotify, rec_track_ids)\n",
    "        \n",
    "rec_playlist_df = pd.DataFrame(rec_features, index=rec_track_names)\n",
    "rec_playlist_df.drop_duplicates(subset='id', inplace=True)\n",
    "rec_track_names = rec_playlist_df.index.tolist()\n",
    "rec_playlist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = rec_playlist_df[\n",
    "    [\n",
    "        \"acousticness\", \"danceability\", \"duration_ms\", \"energy\", \n",
    "        \"instrumentalness\",  \"key\", \"liveness\", \"loudness\", \"mode\", \n",
    "        \"speechiness\", \"tempo\", \"valence\"\n",
    "    ]\n",
    "]\n",
    "testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = [knc_grid, forest_grid, tree_grid]\n",
    "\n",
    "testing_df_scaled = StandardScaler().fit_transform(testing_df)\n",
    "\n",
    "X_test = pca.transform(testing_df_scaled)\n",
    "X_test_names = v.transform(rec_track_names)\n",
    "\n",
    "X_test = sparse.csr_matrix(sparse.hstack([X_test, X_test_names]))\n",
    "y_pred_final = np.array([1] * X_test_names.shape[0])\n",
    "\n",
    "for estimator in estimators:\n",
    "    estimator.best_estimator_.fit(X_train, y_train)\n",
    "    y_pred = estimator.best_estimator_.predict(X_test)\n",
    "    \n",
    "    y_pred_final = y_pred_final * y_pred\n",
    "    print(\"Number of disliked tracks by model: \", sum(y_pred == 0))\n",
    "    print(\"Number of disliked tracks: \", sum(y_pred_final == 0))\n",
    "    print(\"Number of liked tracks: \", sum(y_pred_final == 1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tracks = testing_df[y_pred_final.astype(bool)]\n",
    "final_tracks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
